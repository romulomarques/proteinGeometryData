{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORcMnp3K9UH3",
        "outputId": "acbb19ba-6b2a-451f-dbf1-0e3ae5fbb0a7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# wd = '/mnt/michael/gitrepos/rs_FERNANDO/RFOO'\n",
        "wd = '/home/romulosmarques/pythonProjects/proteinGeometryData'\n",
        "wd_pdb = os.path.join(wd, 'pdb')\n",
        "wd_backbone = os.path.join(wd, 'backbone')\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "wd_binary = os.path.join(wd, 'binary')\n",
        "wd_sol = os.path.join(wd, 'original_sol')\n",
        "wd_nmr = os.path.join(wd, 'nmr')\n",
        "\n",
        "os.chdir(wd)\n",
        "\n",
        "# !pip install biopandas\n",
        "# !pip freeze > requirements.txt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output(wait=True)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting up the plot style\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbTbmMrb8IWi",
        "outputId": "b21fd550-bc71-4932-f3da-44d08588be93"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# URL of the file to download\n",
        "fn = 'pdb_entry_type.txt'\n",
        "url = \"https://files.wwpdb.org/pub/pdb/derived_data/\" + fn\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(os.path.join(wd,fn), 'wb') as fd:\n",
        "        fd.write(response.content)\n",
        "else:\n",
        "    raise Exception(f'Failed to download {url}: {response.status_code}')\n",
        "\n",
        "!head -n 5 pdb_entry_type.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBKLLiD29JAX",
        "outputId": "3b7dbfb9-752b-46f4-fe9c-fec22e4f98ed"
      },
      "outputs": [],
      "source": [
        "# convert from txt to pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read the file into a pandas DataFrame\n",
        "# Assuming the file is tab-delimited and has no header row\n",
        "df = pd.read_csv(os.path.join(wd,fn), delimiter='\\t', header=None)\n",
        "\n",
        "# Optionally, name the columns\n",
        "df.columns = ['Protein_ID', 'Type', 'Method']\n",
        "\n",
        "# Filter Type='prot' and Method='NMR'\n",
        "df = df[(df['Type'] == 'prot') & (df['Method'] == 'NMR')].copy()\n",
        "\n",
        "df['Count'] = 1\n",
        "df_total = df[['Type','Method','Count']].groupby(['Type','Method']).sum().reset_index()\n",
        "df_total = df_total[['Method','Count']].copy()\n",
        "df_total\n",
        "\n",
        "# Export the df_total DataFrame to LaTeX\n",
        "with open(os.path.join(wd,'df_total.tex'), 'w') as tf:\n",
        "    tf.write(df_total.to_latex(index=False))\n",
        "\n",
        "# Export the df DataFrame to LaTeX\n",
        "df.to_csv(os.path.join(wd,'pdb_entry_prot_NMR.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPCJvhXfCVyM",
        "outputId": "dbaa2da8-f076-4951-c8b2-2dd8dc9fccab"
      },
      "outputs": [],
      "source": [
        "# Download PDB files\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "# Read the selected_protein_ids.csv file\n",
        "selected_protein_ids = pd.read_csv(os.path.join(wd,'pdb_entry_prot_NMR.csv'))\n",
        "selected_protein_ids = selected_protein_ids['Protein_ID'].tolist()\n",
        "\n",
        "# Get the number of available CPU cores\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Set the number of threads to be one less than the number of cores\n",
        "num_threads = num_cores - 1\n",
        "\n",
        "# Create a directory to store the downloaded PDB files\n",
        "os.makedirs(wd_pdb, exist_ok=True)\n",
        "\n",
        "# Function to download a PDB file\n",
        "def download_pdb(protein_id):\n",
        "    file_path = os.path.join(wd_pdb, f'{protein_id}.pdb')\n",
        "\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"{protein_id}.pdb already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    url = f'http://files.rcsb.org/download/{protein_id}.pdb'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Successfully downloaded {protein_id}.pdb\")\n",
        "    else:\n",
        "        print(f\"Failed to download {protein_id}.pdb\")\n",
        "\n",
        "# Download PDB files for the selected Protein_IDs in parallel\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    executor.map(download_pdb, selected_protein_ids)\n",
        "\n",
        "# clear_output()\n",
        "\n",
        "# pdb_files = [fn for fn in os.listdir(wd_pdb)]\n",
        "# len(pdb_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3hC-UkomDcK"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "import biopandas.pdb as bp\n",
        "from tqdm import tqdm\n",
        "\n",
        "def detect_invalid_residue(df_row):\n",
        "    residue_name = df_row[1]\n",
        "\n",
        "    if residue_name in ['PRO', 'GLY']:\n",
        "        return False\n",
        "\n",
        "    atoms = df_row[2].split(' ')\n",
        "    if \"N\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"CA\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"C\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"H\" not in atoms and \"H1\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"HA\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def process_pdb_file(fn):\n",
        "    # Define the PDB file path\n",
        "    pdb_id = fn.split('.')[0]\n",
        "    pdb_fn = os.path.join(wd, 'pdb', fn)\n",
        "\n",
        "    # Load PDB file\n",
        "    ppdb = bp.PandasPdb().read_pdb(pdb_fn)\n",
        "\n",
        "    # Get model indices\n",
        "    df_MODEL = ppdb.get_model_start_end().copy()\n",
        "\n",
        "    ATOM = ['C','CA','N','H','H1','HA','HA1']\n",
        "    COLUMNS = [\n",
        "        'atom_number',\n",
        "        'atom_name',\n",
        "        'residue_name',\n",
        "        'chain_id',\n",
        "        'residue_number',\n",
        "        'x_coord',\n",
        "        'y_coord',\n",
        "        'z_coord',\n",
        "        'b_factor',\n",
        "        'segment_id',\n",
        "        'element_symbol',\n",
        "        'model_id'\n",
        "    ]\n",
        "    for _, row in df_MODEL.iterrows():\n",
        "        model_idx = int(row.model_idx)\n",
        "        model = ppdb.get_model(model_index=model_idx).df['ATOM']\n",
        "        # Get only C, CA, N\n",
        "        model = model[model['atom_name'].isin(ATOM)]\n",
        "        chains = sorted(list(set(model['chain_id'])))\n",
        "        if len(chains) > 0:\n",
        "            chain = chains[0] # only one chain \n",
        "            df = model[model['chain_id'] == chain]                                                                                                                                                                                                                                                              .copy()\n",
        "\n",
        "            # removing invalid residues\n",
        "            col_residue_atoms = df.groupby(['residue_number']).agg({'residue_name': 'min', 'atom_name': lambda x : ' '.join(x)}).reset_index()\n",
        "            col_residue_atoms['is_valid'] = col_residue_atoms.apply(detect_invalid_residue, axis=1)\n",
        "            \n",
        "            valid_residues = col_residue_atoms[col_residue_atoms['is_valid']]['residue_number']\n",
        "            valid_residues = set(valid_residues)\n",
        "\n",
        "            df = df[df['residue_number'].isin(valid_residues)]\n",
        "            \n",
        "            fn = os.path.join(wd_backbone, f'{pdb_id}_model{model_idx}_chain{chain}.csv')\n",
        "            df.to_csv(fn, index=False)\n",
        "            break  \n",
        "\n",
        "\n",
        "# df_count_res_atoms = df.value_counts(\"residue_number\", sort=False).reset_index()\n",
        "# col_count = []\n",
        "# for _, row in df_count_res_atoms.iterrows():\n",
        "#     n_atoms = int(row[1])\n",
        "#     if n_atoms < 5:\n",
        "#         for i in range(n_atoms):PÃ¶\n",
        "#             col_count.append(False)\n",
        "#     else:\n",
        "#         for i in range(n_atoms):\n",
        "#             col_count.append(True)\n",
        "\n",
        "# Set up working directories\n",
        "wd_pdb = os.path.join(wd, 'pdb')\n",
        "wd_backbone = os.path.join(wd, 'backbone')\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(wd_backbone, exist_ok=True)\n",
        "\n",
        "# Get list of PDB files\n",
        "pdb_files = sorted(os.listdir(wd_pdb))\n",
        "\n",
        "# Number of threads (adjust as needed)\n",
        "num_threads = os.cpu_count() - 1\n",
        "\n",
        "# for fn in tqdm(pdb_files):\n",
        "#     process_pdb_file(fn)\n",
        "\n",
        "#part_pdb_files = pdb_files[8291:len(pdb_files)].copy() #list(range(8291, len(pdb_files)))\n",
        "# Parallel processing\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    list(tqdm(executor.map(process_pdb_file, pdb_files), total=len(pdb_files)))\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz1OrnLbqDaY",
        "outputId": "fe102c96-6eec-4e65-f5db-bb6fcfb9f3c4"
      },
      "outputs": [],
      "source": [
        "# Create segments\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(wd_segment, exist_ok=True)\n",
        "\n",
        "cols = [\n",
        "    'residue_number',\n",
        "    'residue_name',\n",
        "    'atom_number',\n",
        "    'atom_name',\n",
        "    'chain_id',\n",
        "    'x_coord',\n",
        "    'y_coord',\n",
        "    'z_coord'\n",
        "]\n",
        "\n",
        "for fn in tqdm(os.listdir(wd_backbone)):\n",
        "    df = pd.read_csv(os.path.join(wd_backbone, fn))\n",
        "    df = df[cols]\n",
        "\n",
        "    # Assume df is your original DataFrame\n",
        "    df_residue = df.groupby(['residue_number']).max().reset_index()\n",
        "\n",
        "    # Convert the 'residue_number' column to a NumPy array\n",
        "    residue_number = df_residue['residue_number'].to_numpy()\n",
        "\n",
        "    # # Selecting the indices of the proline and glycine residues\n",
        "    # pro_gly_indices = df_residue[(df_residue['residue_name'] == 'PRO') | (df_residue['residue_name'] == 'GLY')].index.to_numpy()\n",
        "    # print(pro_gly_indices)\n",
        "\n",
        "    # Calculate the difference between consecutive entries in the residue_number array\n",
        "    diffs = np.diff(residue_number)\n",
        "\n",
        "    # Identify indices where the difference is larger than 1\n",
        "    split_indices = np.where(diffs > 1)[0] + 1\n",
        "    # print(split_indices)\n",
        "\n",
        "    # # Combining the indices of non-consective residues with proline and glycine residues\n",
        "    # if not split_indices.any():\n",
        "    #     split_indices = pro_gly_indices\n",
        "    # elif pro_gly_indices != []:\n",
        "    #     split_indices = sorted(split_indices + pro_gly_indices)\n",
        "    \n",
        "    # Split the array at these indices\n",
        "    split_arrays = np.split(residue_number, split_indices)\n",
        "\n",
        "    # Keeping segments which has at least 3 residues.\n",
        "    cleaned_split_arrays = []\n",
        "    for index, array in enumerate(split_arrays):\n",
        "        if len(array) > 3:\n",
        "            cleaned_split_arrays.append(split_arrays[index])\n",
        "    \n",
        "    fn = fn.replace('.csv','')\n",
        "    for k, residue_numbers in enumerate(cleaned_split_arrays):\n",
        "        df_segment = df[df['residue_number'].isin(residue_numbers)]\n",
        "\n",
        "        # Removing the amina hydrogen from the first residue\n",
        "        row_firstH = df_segment.loc[(df_segment['residue_number'] == residue_numbers[0]) & ((df_segment['atom_name'] == 'H') | (df_segment['atom_name'] == 'H1'))]\n",
        "        df_segment = df_segment.drop(row_firstH.index)\n",
        "        \n",
        "        fn_segment = os.path.join(wd_segment, f'{fn}_segment{k}.csv')\n",
        "        df_segment.to_csv(fn_segment, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpS3BJKcOHHk"
      },
      "outputs": [],
      "source": [
        "# create binary strings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial import distance\n",
        "\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "\n",
        "def point_plane_distance(point, plane_points):\n",
        "    \"\"\"\n",
        "    Calculate the signed distance from a point to a plane defined by three points.\n",
        "    Parameters:\n",
        "        point (numpy array): The point [x, y, z] we want to check.\n",
        "        plane_points (numpy array): 3x3 array where each row is a point [x, y, z] defining the plane.\n",
        "    Returns:\n",
        "        float: The signed distance from the point to the plane.\n",
        "    \"\"\"\n",
        "    # Calculate the normal vector of the plane\n",
        "    normal_vector = np.cross(plane_points[1] - plane_points[0], plane_points[2] - plane_points[0])\n",
        "    \n",
        "    norm_nv = np.linalg.norm(normal_vector)\n",
        "    \n",
        "    if norm_nv == 0:\n",
        "        raise Exception(\"Normal vector is null!\")\n",
        "\n",
        "    # Calculate the signed distance\n",
        "    signed_distance = np.dot(normal_vector, point - plane_points[0]) / norm_nv\n",
        "    \n",
        "    return signed_distance\n",
        "\n",
        "\n",
        "def construct_ddgp_order_1(df):\n",
        "    \"\"\"\n",
        "    Constructs the following ddgp order: N^1, CA^1, HA^1, ..., C^{i-1}, H^i, N^i, CA^i, HA^i, ..., C^n, where n is the number of vertices of the ddgp graph.\n",
        "    Parameters:\n",
        "        df (DataFrame): DataFrame containing the 'x' column with 3D coordinates.\n",
        "    Returns:\n",
        "        order_1 (numpy array): nx4 array containing:\n",
        "            (a) in the first column, the DataFrame indices of the elements of the proposed 3-DDGP order_1; \n",
        "            (b) in the other three columns, the neigbours which are utilized to emerge the vertices in the R^3;\n",
        "                order_1[i][1] is closer to order_1[i][0] in the order than order_1[i][2] is, and so on.    \n",
        "    \"\"\"\n",
        "\n",
        "    # df['myind'] = [str(i) for i in range(len(df))]\n",
        "    df = df.reset_index()\n",
        "    df['myindex'] = df['index'].apply(lambda x : str(x))\n",
        "    df_residue_info = df.groupby(['residue_number']).agg({'residue_name': 'min', 'atom_name': lambda x : ' '.join(x), 'myindex': lambda y : ' '.join(y)}).reset_index()\n",
        "    df = df.drop(['myindex'], axis=1)\n",
        "\n",
        "    # atom_names_im1 = df_residue_info.iloc[0][2].split(' ')\n",
        "    atom_names_im1 = df_residue_info.iloc[0].iloc[2].split(' ')\n",
        "    # atom_indexes_im1 = [int(atom_index) for atom_index in df_residue_info.iloc[0][3].split(' ')]\n",
        "    atom_indexes_im1 = [int(atom_index) for atom_index in df_residue_info.iloc[0].iloc[3].split(' ')]\n",
        "    atoms_im1 = {atom_names_im1[k]: atom_indexes_im1[k] for k in range(len(atom_names_im1))}\n",
        "\n",
        "    # Creating the order_1\n",
        "    order_1 = np.zeros((len(df), 4))\n",
        "\n",
        "    # The first three elements of the order\n",
        "    order_1[0][0] = atoms_im1['N']\n",
        "    order_1[1][0] = atoms_im1['CA']\n",
        "    order_1[2][0] = atoms_im1['HA']\n",
        "\n",
        "    # The first three elements of the order are not calculated using previous neighbours.\n",
        "    order_1[0][1] = np.nan\n",
        "    order_1[1][1] = np.nan\n",
        "    order_1[2][1] = np.nan\n",
        "    order_1[0][2] = np.nan\n",
        "    order_1[1][2] = np.nan\n",
        "    order_1[2][2] = np.nan\n",
        "    order_1[0][3] = np.nan\n",
        "    order_1[1][3] = np.nan\n",
        "    order_1[2][3] = np.nan\n",
        "\n",
        "    current_i = 3\n",
        "    for ind, row in df_residue_info.iloc[1:].iterrows():\n",
        "        #atom_names_i = row[2].split(' ')\n",
        "        atom_names_i = row.iloc[2].split(' ')\n",
        "        # atom_indexes_i = [int(atom_index) for atom_index in row[3].split(' ')]\n",
        "        atom_indexes_i = [int(atom_index) for atom_index in row.iloc[3].split(' ')]\n",
        "        atoms_i = {atom_names_i[k]: atom_indexes_i[k] for k in range(len(atom_names_i))}\n",
        "\n",
        "        ################################\n",
        "        \n",
        "        if 'C' in atoms_im1:\n",
        "            order_1[current_i][0] = atoms_im1['C']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'C' does not exist in the %d-th residue!\" % ind-1)\n",
        "\n",
        "        order_1[current_i][1] = atoms_im1['N']\n",
        "        order_1[current_i][2] = atoms_im1['CA']\n",
        "        order_1[current_i][3] = atoms_im1['HA']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'H' in atoms_i:\n",
        "            order_1[current_i + 1][0] = atoms_i['H']\n",
        "        elif 'H1' in atoms_i:\n",
        "                order_1[current_i + 1][0] = atoms_i['H1']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'H' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 1][1] = atoms_im1['CA']\n",
        "        order_1[current_i + 1][2] = atoms_im1['C']\n",
        "        order_1[current_i + 1][3] = atoms_im1['HA']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'N' in atoms_i:\n",
        "            order_1[current_i + 2][0] = atoms_i['N']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'N' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 2][1] = atoms_im1['CA']\n",
        "        order_1[current_i + 2][2] = atoms_im1['C']\n",
        "        order_1[current_i + 2][3] = atoms_i['H']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'CA' in atoms_i:\n",
        "            order_1[current_i + 3][0] = atoms_i['CA']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'CA' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 3][1] = atoms_im1['C']\n",
        "        order_1[current_i + 3][2] = atoms_i['N']\n",
        "        order_1[current_i + 3][3] = atoms_i['H']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'HA' in atoms_i:\n",
        "            order_1[current_i + 4][0] = atoms_i['HA']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'HA' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 4][1] = atoms_i['N']\n",
        "        order_1[current_i + 4][2] = atoms_i['CA']\n",
        "        order_1[current_i + 4][3] = atoms_i['H']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        atom_names_im1 = atom_names_i.copy()\n",
        "        atom_indexes_im1 = atom_indexes_i.copy()\n",
        "        atoms_im1 = atoms_i.copy()\n",
        "        \n",
        "        current_i = current_i + 5\n",
        "    \n",
        "    # The last element of the order\n",
        "    if 'C' in atoms_i:\n",
        "        order_1[current_i][0] = atoms_im1['C']\n",
        "    else:\n",
        "        raise Exception(\"Atom 'C' does not exist in the %d-th residue!\" % ind)\n",
        "    \n",
        "    # The neighbours of the last element\n",
        "    order_1[current_i][1] = atoms_i['N']\n",
        "    order_1[current_i][2] = atoms_i['CA']\n",
        "    order_1[current_i][3] = atoms_i['HA']\n",
        "\n",
        "    return order_1\n",
        "\n",
        "\n",
        "def calculate_b_column(df, order=[]):\n",
        "    \"\"\"\n",
        "    Calculate the binary column 'b' based on the 'x' column of the DataFrame.\n",
        "    Parameters:\n",
        "        df (DataFrame): DataFrame containing the 'x' column with 3D coordinates.\n",
        "        order (numpy array): nx4 array containing:\n",
        "            (a) in the first column, the DataFrame indices of the elements of the proposed 3-DDGP order_1; \n",
        "            (b) in the other three columns, the neigbours which are utilized to emerge the vertices in the R^3;\n",
        "                order_1[i][1] is closer to order_1[i][0] in the order than order_1[i][2] is, and so on.\n",
        "    Returns:\n",
        "        Series: Pandas Series containing the binary values for the 'b' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # When no order is passed, we assume the DataFrame order is a 3-DMDGP order\n",
        "    if len(order) == 0:\n",
        "        col0 = list(range(len(df)))\n",
        "        col1 = list(range(-3, len(df)-3))\n",
        "        col2 = list(range(-2, len(df)-2))\n",
        "        col3 = list(range(-1, len(df)-1))\n",
        "        col1[0:3] = [np.nan, np.nan, np.nan]\n",
        "        col2[0:3] = [np.nan, np.nan, np.nan]\n",
        "        col3[0:3] = [np.nan, np.nan, np.nan]\n",
        "        order = np.array([col0, col1, col2, col3,])\n",
        "        order = np.transpose(order)\n",
        "\n",
        "    b_values = []\n",
        "    for i in range(len(df)):\n",
        "        if i < 3:\n",
        "            # Not enough points to define a plane\n",
        "            b_values.append(None)\n",
        "            continue\n",
        "        # b = order[i][1:4].astype(int)\n",
        "        plane_points = np.array(df.iloc[order[i][1:4].astype(int)]['x'].tolist())\n",
        "        # a = order[i][0]\n",
        "        current_point = df.iloc[order[i][0].astype(int)]['x']\n",
        "        distance_to_plane = point_plane_distance(current_point, plane_points)\n",
        "        b_values.append(int(distance_to_plane >= 0))\n",
        "    return pd.Series(b_values)\n",
        "\n",
        "\n",
        "def flip_b_column_if_needed(b_col):\n",
        "    \"\"\"\n",
        "    Flip the binary column 'b' if the 4th element (index 3) is 1.\n",
        "    Parameters:\n",
        "        b_col (Series): Panda Series containing the 'b' column.\n",
        "    Returns:\n",
        "        Series: Pandas Series containing the binary values for the 'b' column flipped around the fourth position.\n",
        "    \"\"\"\n",
        "    if len(b_col) > 3 and b_col.iloc[3] == 1:\n",
        "        # b_col = [1 - x if x is not None else None for x in b_col]\n",
        "        b_col = b_col.apply(lambda x: 1 - x if x is not None else None)\n",
        "    return b_col\n",
        "\n",
        "\n",
        "def write_binary(fn_binary, df, order, b_col):\n",
        "    df_binary = df[['residue_number', 'atom_name']].copy()\n",
        "    indices = np.zeros([order.shape[0], 1])\n",
        "    for i in range(order.shape[0]):\n",
        "        indices[int(order[i][0])] = i\n",
        "    df_binary['order'] = indices\n",
        "    df_binary.sort_values(by=['order'], inplace=True)\n",
        "    df_binary.drop(\"order\", axis='columns', inplace=True)\n",
        "\n",
        "    # Guarantees that new columns will not be sorted\n",
        "    df_binary.reset_index(inplace=True)\n",
        "    df_binary.rename(columns={'index': 'order'}, inplace=True)\n",
        "\n",
        "    df_binary['N_1'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 1].astype(int)))\n",
        "    df_binary['N_2'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 2].astype(int)))\n",
        "    df_binary['N_3'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 3].astype(int)))\n",
        "    df_binary['b'] = b_col\n",
        "\n",
        "    df_binary.to_csv(fn_binary, index=False)\n",
        "\n",
        "\n",
        "os.makedirs(wd_binary, exist_ok=True)\n",
        "\n",
        "for fn in tqdm(os.listdir(wd_segment)):\n",
        "    try:\n",
        "        fn_segment = os.path.join(wd_segment, fn)\n",
        "        fn_binary = os.path.join(wd_binary, fn.split('.')[0] + '_binary' + '.csv')\n",
        "        df = pd.read_csv(fn_segment)\n",
        "        # print()\n",
        "        # print(\"XXXXXXXXXXXXXXXXXXXXXX\")\n",
        "        # print()\n",
        "        order_1 = construct_ddgp_order_1(df)\n",
        "        # Create col x\n",
        "        df['x'] = df.apply(lambda row: np.array([row['x_coord'], row['y_coord'], row['z_coord']]), axis=1)\n",
        "        # Calculate the 'b' column\n",
        "        b_col = calculate_b_column(df, order_1)\n",
        "        b_col = flip_b_column_if_needed(b_col)\n",
        "        write_binary(fn_binary, df, order_1, b_col)\n",
        "    except Exception as myexception:\n",
        "        print('File %s: ' %fn + myexception.args[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitsB8R_8ITr",
        "outputId": "6c8264a1-aa9f-47fd-8cae-33b1cf9aa020"
      },
      "outputs": [],
      "source": [
        "# wd_binary = os.path.join(wd, 'binary')\n",
        "M = {'fn':[], 'n':[], 's':[], 'order': [], 'N_1': [], 'N_2': [], 'N_3': []}\n",
        "\n",
        "for fn in tqdm(os.listdir(wd_binary)):\n",
        "    fn = os.path.join(wd_binary, fn)\n",
        "    df = pd.read_csv(fn)\n",
        "    # convert from b to s\n",
        "    s = ''.join(df['b'].dropna().astype(int).astype(str))\n",
        "    s = s[1:] # remove b_4\n",
        "    order = ''.join(df['order'].astype(int).astype(str))\n",
        "    N_1 = ' '.join(df['N_1'].dropna().astype(int).astype(str))\n",
        "    N_2 = ' '.join(df['N_2'].dropna().astype(int).astype(str))\n",
        "    N_3 = ' '.join(df['N_3'].dropna().astype(int).astype(str))\n",
        "    M['fn'].append(fn)\n",
        "    M['s'].append(s)\n",
        "    M['n'].append(len(s))\n",
        "    M['order'].append(order)\n",
        "    M['N_1'].append(N_1)\n",
        "    M['N_2'].append(N_2)\n",
        "    M['N_3'].append(N_3)\n",
        "\n",
        "M = pd.DataFrame(M)\n",
        "M.to_csv('df_strings.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "XlByzSBUASFt",
        "outputId": "97e1eae8-286d-4cd3-f721-6a2c7e923f4f"
      },
      "outputs": [],
      "source": [
        "M = pd.read_csv(os.path.join(wd, 'df_strings.csv'))\n",
        "N = M[['n','s']].groupby(['n']).count().reset_index()\n",
        "N.rename(columns={'s': 'count'}, inplace=True)\n",
        "\n",
        "# Re-creating a Kernel Density Estimate (KDE) plot for the 'n' distribution\n",
        "plt.figure(figsize=(6, 3))\n",
        "sns.kdeplot(N['n'], bw_adjust=1, fill=True)\n",
        "plt.title(\"Density Estimate of Reduced String Lengths\")\n",
        "plt.xlabel('n')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HySmxeu0ATel",
        "outputId": "3f99c856-6d7e-4a20-c347-2483c1016538"
      },
      "outputs": [],
      "source": [
        "# create *.sol files\n",
        "import re\n",
        "\n",
        "# extract atoms coordinates\n",
        "def extract_coordinates(df):\n",
        "    if len(df['x']) > 0:\n",
        "        df['x'] = df['x'].apply(lambda x: eval(re.sub(\" +\", \",\", re.sub(r\"\\[\\s+\",\"[\", x))))\n",
        "        x = np.array(df['x'].to_list())\n",
        "        return x\n",
        "    return None\n",
        "\n",
        "os.makedirs(wd_sol, exist_ok=True)\n",
        "\n",
        "# OBS: The files of the 'binery' folder represent the solvable ddgp problems.\n",
        "# Since the files of the 'segment' folder might not have a correspondent file in the 'binary' folder,\n",
        "# we iterate over the 'binary' files.\n",
        "for fn in tqdm(M['fn']):\n",
        "    fn_segment = os.path.join(wd_segment, fn.split('/')[-1])\n",
        "    fn_segment = fn_segment.replace('_binary.csv','.csv')\n",
        "    fn_sol = os.path.join(wd_sol, fn_segment.split('/')[-1])\n",
        "    fn_sol = fn_sol.replace('.csv','.sol')\n",
        "    if os.path.exists(fn_sol):\n",
        "        continue\n",
        "    # df_binary = pd.read_csv(fn)\n",
        "    df = pd.read_csv(fn_segment)\n",
        "    df['x'] = df.apply(lambda row: str(np.array([row['x_coord'], row['y_coord'], row['z_coord']])), axis=1)\n",
        "    x = extract_coordinates(df)\n",
        "    np.savetxt(fn_sol, x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count fixed-size binary pieces.\n",
        "import math\n",
        "\n",
        "def flip_b_column_if_needed(b_col):\n",
        "    \"\"\"\n",
        "    Flip the binary column 'b' if the 4th element (index 3) is 1.\n",
        "    Parameters:\n",
        "        b_col (Series): Panda Series containing the 'b' column.\n",
        "    Returns:\n",
        "        Series: Pandas Series containing the binary values for the 'b' column flipped around the fourth position.\n",
        "    \"\"\"\n",
        "    if len(b_col) > 3 and b_col.iloc[3] == 1:\n",
        "        # b_col = [1 - x if x is not None else None for x in b_col]\n",
        "        b_col = b_col.apply(lambda x: 1 - x if x is not None else None)\n",
        "    return b_col\n",
        "\n",
        "res_quantities = [3, 4]\n",
        "n_atoms_per_res = 5\n",
        "slice_sizes = [n_atoms_per_res * res_quantities[i] for i in range(len(res_quantities))]\n",
        "\n",
        "slices = {str(slice_sizes[i]): [] for i in range(len(slice_sizes))}\n",
        "for binary in tqdm(M['s']):\n",
        "    for size in slice_sizes:\n",
        "        a = len(binary)\n",
        "        n_current_slices = math.floor((len(binary) - size)/n_atoms_per_res) + 1\n",
        "        slice_starts = [i * n_atoms_per_res for i in range(n_current_slices)]\n",
        "\n",
        "        # do the first case here\n",
        "        if (len(binary) < size):\n",
        "            # raise Exception('Binary has length %d, which is lesser then the size of the current slice (%d)' % (len(binary), size))\n",
        "            continue\n",
        "        slices[str(size)].append(binary[slice_starts[0]:(slice_starts[0] + size)])\n",
        "        \n",
        "        for start in slice_starts[1:]:\n",
        "            b_str = binary[(start - 4):(start + size)]\n",
        "            b_series = pd.Series([int(b_str[i]) for i in range(len(b_str))])\n",
        "            \n",
        "            b_series_flipped = flip_b_column_if_needed(b_series)\n",
        "            b_str_flipped = ''.join(b_series_flipped.astype(str))\n",
        "            b_str_flipped = b_str_flipped[4:]\n",
        "\n",
        "            slices[str(size)].append(b_str_flipped)\n",
        "\n",
        "df = {'b': [], 'size': []}\n",
        "for size in slices:\n",
        "    df['b'] = df['b'] + slices[size]\n",
        "    df['size'] = df['size'] + [len(slices[size][i]) for i in range(len(slices[size]))]\n",
        "    # df['size'] = df['size'] + ([int(size)] * len(slices[size]))\n",
        "\n",
        "df = pd.DataFrame.from_dict(df)\n",
        "\n",
        "df_count_slices = df.groupby(['b', 'size'], sort=True).size()\n",
        "df_count_slices = df_count_slices.rename('count').reset_index()\n",
        "df_count_slices['size'] = df_count_slices['size'].apply(lambda x : -x)\n",
        "df_count_slices.sort_values(by=['size', 'count'], inplace=True, ascending=False)\n",
        "df_count_slices['size'] = df_count_slices['size'].apply(lambda x : -x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0-9CM5jICoV",
        "outputId": "472e2f31-d8a0-4ff0-a3ee-837623637b5f"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Function to create and prune edges based on distance\n",
        "# , order, N_1, N_2, N_3\n",
        "def create_prune_edges(fn, dmax=5):\n",
        "    Ep = {'i': [], 'j': [], 'dij': []}\n",
        "    fn_nmr = fn.replace('.csv', '.nmr')\n",
        "\n",
        "    # Skip if file already exists\n",
        "    if os.path.exists(fn_nmr):\n",
        "        return\n",
        "\n",
        "    fn_sol = fn.replace('.csv', '.sol')\n",
        "    x = np.loadtxt(fn_sol)\n",
        "\n",
        "    for i in range(0, len(x)):\n",
        "        xi = x[i]\n",
        "        for j in range(i + 4, len(x)):\n",
        "            xj = x[j]\n",
        "            dij = np.linalg.norm(xi - xj)\n",
        "            if dij < dmax:\n",
        "                Ep['i'].append(i)\n",
        "                Ep['j'].append(j)\n",
        "                Ep['dij'].append(dij)\n",
        "\n",
        "    Ep = pd.DataFrame(Ep)\n",
        "    Ep.to_csv(fn_nmr, index=False)\n",
        "\n",
        "# Determine the number of CPU cores\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Use number of cores - 1 for thread count\n",
        "num_threads = num_cores - 1\n",
        "\n",
        "# Using ThreadPoolExecutor to parallelize the function\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # , M['order'], M['N_1'], M['N_2'], M['N_3']\n",
        "    list(tqdm(executor.map(create_prune_edges, M['fn']), total=len(M)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwt4xM-WuLIP",
        "outputId": "eb6fcc27-4dba-47b6-add4-c86a2fb3d320"
      },
      "outputs": [],
      "source": [
        "E = {'fn':[], 'df':[]}\n",
        "for fn in tqdm(os.listdir(wd_segment)):\n",
        "    if not fn.endswith('.nmr'):\n",
        "        continue\n",
        "    fn = os.path.join(wd_segment, fn)\n",
        "    E['fn'].append(fn)\n",
        "    E['df'].append(pd.read_csv(fn))\n",
        "\n",
        "E = pd.DataFrame(E)\n",
        "\n",
        "# remove empty dataframes from E\n",
        "E = E[E['df'].apply(lambda x: len(x) > 0)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "HvcL0_biudmp",
        "outputId": "2c280744-b2e7-48a5-fe5d-04353fc3e8a1"
      },
      "outputs": [],
      "source": [
        "# merge all dataframes on E['df']\n",
        "D = pd.concat(E['df'].values, ignore_index=True)\n",
        "print(f'len(D)={len(D)}')\n",
        "\n",
        "v = D['j'] - D['i']\n",
        "\n",
        "display(v.describe())\n",
        "\n",
        "# using seaborn, create a histogram of the 'v' column\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.histplot(v, bins=100)\n",
        "plt.title(\"Distribution of |j - i|\")\n",
        "plt.xlabel('|j - i|')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# using seaborn, create a boxplot of the 'v' column\n",
        "plt.figure(figsize=(4, 12))\n",
        "sns.boxplot(v)\n",
        "plt.title(\"Distribution of |j - i|\")\n",
        "plt.xlabel('|j - i|')\n",
        "plt.show()\n",
        "\n",
        "# plot the cumulative distribution of the 'v' column\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.histplot(v, bins=1000, cumulative=True)\n",
        "plt.title(\"Cumulative Distribution of |j - i|\")\n",
        "plt.xlabel('|j - i|')\n",
        "plt.ylabel('Count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
