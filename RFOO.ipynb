{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORcMnp3K9UH3",
        "outputId": "acbb19ba-6b2a-451f-dbf1-0e3ae5fbb0a7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "wd = '/home/michael/gitrepos/proteinGeometryData'\n",
        "# wd = 'C:/Users/romul/Documents/PythonProjects/proteinGeometryData'\n",
        "wd_pdb = os.path.join(wd, 'pdb')\n",
        "wd_backbone = os.path.join(wd, 'backbone')\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "wd_binary = os.path.join(wd, 'binary')\n",
        "wd_sol = os.path.join(wd, 'original_sol')\n",
        "wd_nmr = os.path.join(wd, 'nmr')\n",
        "\n",
        "os.chdir(wd)\n",
        "\n",
        "# !pip install biopandas\n",
        "# !pip freeze > requirements.txt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output(wait=True)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Setting up the plot style\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbTbmMrb8IWi",
        "outputId": "b21fd550-bc71-4932-f3da-44d08588be93"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL of the file to download\n",
        "pdb_code = 'pdb_entry_type.txt'\n",
        "url = \"https://files.wwpdb.org/pub/pdb/derived_data/\" + pdb_code\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(os.path.join(wd,pdb_code), 'wb') as fd:\n",
        "        fd.write(response.content)\n",
        "else:\n",
        "    raise Exception(f'Failed to download {url}: {response.status_code}')\n",
        "\n",
        "!head -n 5 pdb_entry_type.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBKLLiD29JAX",
        "outputId": "3b7dbfb9-752b-46f4-fe9c-fec22e4f98ed"
      },
      "outputs": [],
      "source": [
        "# convert from txt to pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read the file into a pandas DataFrame\n",
        "# Assuming the file is tab-delimited and has no header row\n",
        "df = pd.read_csv(os.path.join(wd,pdb_code), delimiter='\\t', header=None)\n",
        "\n",
        "# Optionally, name the columns\n",
        "df.columns = ['Protein_ID', 'Type', 'Method']\n",
        "\n",
        "# Filter Type='prot' and Method='NMR'\n",
        "df = df[(df['Type'] == 'prot') & (df['Method'] == 'NMR')].copy()\n",
        "\n",
        "df['Count'] = 1\n",
        "df_total = df[['Type','Method','Count']].groupby(['Type','Method']).sum().reset_index()\n",
        "df_total = df_total[['Method','Count']].copy()\n",
        "df_total\n",
        "\n",
        "# Export the df_total DataFrame to LaTeX\n",
        "with open(os.path.join(wd,'df_total.tex'), 'w') as tf:\n",
        "    tf.write(df_total.to_latex(index=False))\n",
        "\n",
        "# Export the df DataFrame to LaTeX\n",
        "df.to_csv(os.path.join(wd,'pdb_entry_prot_NMR.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPCJvhXfCVyM",
        "outputId": "dbaa2da8-f076-4951-c8b2-2dd8dc9fccab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Read the selected_protein_ids.csv file\n",
        "selected_protein_ids = pd.read_csv(os.path.join(wd, 'pdb_entry_prot_NMR.csv'))\n",
        "selected_protein_ids = selected_protein_ids['Protein_ID'].tolist()\n",
        "\n",
        "# Get the number of available CPU cores\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Set the number of threads to be one less than the number of cores\n",
        "num_threads = num_cores - 1\n",
        "\n",
        "# Create a directory to store the downloaded PDB files\n",
        "os.makedirs(wd_pdb, exist_ok=True)\n",
        "\n",
        "# Function to download a PDB file\n",
        "def download_pdb(protein_id):\n",
        "    file_path = os.path.join(wd_pdb, f'{protein_id}.pdb')\n",
        "\n",
        "    # Check if the file already exists\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    url = f'http://files.rcsb.org/download/{protein_id}.pdb'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "# Download PDB files for the selected Protein_IDs in parallel with progress bar\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    list(tqdm(executor.map(download_pdb, selected_protein_ids), total=len(selected_protein_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3hC-UkomDcK"
      },
      "outputs": [],
      "source": [
        "# Extracting the backbone chains from the PDB files\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "import biopandas.pdb as bp\n",
        "from tqdm import tqdm\n",
        "\n",
        "def detect_invalid_residue(df_row):\n",
        "    residue_name = df_row.iloc[1]\n",
        "\n",
        "    if residue_name in ['PRO', 'GLY']:\n",
        "        return False\n",
        "\n",
        "    atoms = df_row.iloc[2].split(' ')\n",
        "    if \"N\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"CA\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"C\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"H\" not in atoms and \"H1\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    if \"HA\" not in atoms and \"HA1\" not in atoms:\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def process_pdb_file(fn):\n",
        "    # Define the PDB file path\n",
        "    pdb_id = fn.split('.')[0]\n",
        "    pdb_fn = os.path.join(wd, 'pdb', fn)\n",
        "\n",
        "    # Load PDB file\n",
        "    ppdb = bp.PandasPdb().read_pdb(pdb_fn)\n",
        "\n",
        "    # Get model indices\n",
        "    df_MODEL = ppdb.get_model_start_end().copy()\n",
        "\n",
        "    ATOM = ['C','CA','N','H','H1','HA','HA1']\n",
        "    COLUMNS = [\n",
        "        'atom_number',\n",
        "        'atom_name',\n",
        "        'residue_name',\n",
        "        'chain_id',\n",
        "        'residue_number',\n",
        "        'x_coord',\n",
        "        'y_coord',\n",
        "        'z_coord',\n",
        "        'b_factor',\n",
        "        'segment_id',\n",
        "        'element_symbol',\n",
        "        'model_id'\n",
        "    ]\n",
        "    for _, row in df_MODEL.iterrows():\n",
        "        model_idx = int(row.model_idx)\n",
        "        model = ppdb.get_model(model_index=model_idx).df['ATOM']\n",
        "        # Get only the ATOM records\n",
        "        model = model[model['atom_name'].isin(ATOM)]\n",
        "        chains = sorted(list(set(model['chain_id'])))\n",
        "        if len(chains) > 0:\n",
        "            chain = chains[0] # only one chain \n",
        "            df = model[model['chain_id'] == chain]                                                                                                                                                                                                                                                              .copy()\n",
        "\n",
        "            # removing invalid residues\n",
        "            col_residue_atoms = df.groupby(['residue_number']).agg({'residue_name': 'min', 'atom_name': lambda x : ' '.join(x)}).reset_index()\n",
        "            col_residue_atoms['is_valid'] = col_residue_atoms.apply(detect_invalid_residue, axis=1)\n",
        "            \n",
        "            valid_residues = col_residue_atoms[col_residue_atoms['is_valid']]['residue_number']\n",
        "            valid_residues = set(valid_residues)\n",
        "\n",
        "            df = df[df['residue_number'].isin(valid_residues)]\n",
        "            \n",
        "            fn = os.path.join(wd_backbone, f'{pdb_id}_model{model_idx}_chain{chain}.csv')\n",
        "            df.to_csv(fn, index=False)\n",
        "            break  \n",
        "\n",
        "\n",
        "# Set up working directories\n",
        "wd_pdb = os.path.join(wd, 'pdb')\n",
        "wd_backbone = os.path.join(wd, 'backbone')\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(wd_backbone, exist_ok=True)\n",
        "\n",
        "# Get list of PDB files\n",
        "pdb_files = sorted(os.listdir(wd_pdb))\n",
        "\n",
        "# Number of threads (adjust as needed)\n",
        "num_threads = os.cpu_count() - 1\n",
        "\n",
        "# Parallel processing\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    list(tqdm(executor.map(process_pdb_file, pdb_files), total=len(pdb_files)))\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz1OrnLbqDaY",
        "outputId": "fe102c96-6eec-4e65-f5db-bb6fcfb9f3c4"
      },
      "outputs": [],
      "source": [
        "# Create segments\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(wd_segment, exist_ok=True)\n",
        "\n",
        "cols = [\n",
        "    'residue_number',\n",
        "    'residue_name',\n",
        "    'atom_number',\n",
        "    'atom_name',\n",
        "    'chain_id',\n",
        "    'x_coord',\n",
        "    'y_coord',\n",
        "    'z_coord'\n",
        "]\n",
        "\n",
        "for pdb_code in tqdm(os.listdir(wd_backbone)):\n",
        "    df = pd.read_csv(os.path.join(wd_backbone, pdb_code))\n",
        "    df = df[cols]\n",
        "\n",
        "    # Assume df is your original DataFrame\n",
        "    df_residue = df.groupby(['residue_number']).max().reset_index()\n",
        "\n",
        "    # Convert the 'residue_number' column to a NumPy array\n",
        "    residue_number = df_residue['residue_number'].to_numpy()\n",
        "\n",
        "    # Calculate the difference between consecutive entries in the residue_number array\n",
        "    diffs = np.diff(residue_number)\n",
        "\n",
        "    # Identify indices where the difference is larger than 1\n",
        "    split_indices = np.where(diffs > 1)[0] + 1\n",
        "    \n",
        "    # Split the array at these indices\n",
        "    split_arrays = np.split(residue_number, split_indices)\n",
        "\n",
        "    # Keeping segments which has at least 3 residues.\n",
        "    cleaned_split_arrays = []\n",
        "    for index, array in enumerate(split_arrays):\n",
        "        if len(array) > 3:\n",
        "            cleaned_split_arrays.append(split_arrays[index])\n",
        "    \n",
        "    pdb_code = pdb_code.replace('.csv','')\n",
        "    for k, residue_numbers in enumerate(cleaned_split_arrays):\n",
        "        df_segment = df[df['residue_number'].isin(residue_numbers)]\n",
        "\n",
        "        # Removing the amina hydrogen from the first residue\n",
        "        row_firstH = df_segment.loc[(df_segment['residue_number'] == residue_numbers[0]) & ((df_segment['atom_name'] == 'H') | (df_segment['atom_name'] == 'H1'))]\n",
        "        df_segment = df_segment.drop(row_firstH.index)\n",
        "        \n",
        "        fn_segment = os.path.join(wd_segment, f'{pdb_code}_segment{k}.csv')\n",
        "        df_segment.to_csv(fn_segment, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IpS3BJKcOHHk"
      },
      "outputs": [],
      "source": [
        "# create binary strings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial import distance\n",
        "\n",
        "wd_segment = os.path.join(wd, 'segment')\n",
        "\n",
        "def point_plane_distance(point, plane_points):\n",
        "    \"\"\"\n",
        "    Calculate the signed distance from a point to a plane defined by three points.\n",
        "    Parameters:\n",
        "        point (numpy array): The point [x, y, z] we want to check.\n",
        "        plane_points (numpy array): 3x3 array where each row is a point [x, y, z] defining the plane.\n",
        "    Returns:\n",
        "        float: The signed distance from the point to the plane.\n",
        "    \"\"\"\n",
        "    # Calculate the normal vector of the plane\n",
        "    normal_vector = np.cross(plane_points[1] - plane_points[0], plane_points[2] - plane_points[0])\n",
        "    \n",
        "    norm_nv = np.linalg.norm(normal_vector)\n",
        "    \n",
        "    if norm_nv == 0:\n",
        "        raise Exception(\"Normal vector is null!\")\n",
        "\n",
        "    # Calculate the signed distance\n",
        "    signed_distance = np.dot(normal_vector, point - plane_points[0]) / norm_nv\n",
        "    \n",
        "    return signed_distance\n",
        "\n",
        "\n",
        "def construct_ddgp_order_1(df):\n",
        "    \"\"\"\n",
        "    Constructs the following ddgp order: N^1, CA^1, HA^1, ..., C^{i-1}, H^i, N^i, CA^i, HA^i, ..., C^n, where n is the number of vertices of the ddgp graph.\n",
        "    Parameters:\n",
        "        df (DataFrame): DataFrame containing the 'x' column with 3D coordinates.\n",
        "    Returns:\n",
        "        order_1 (numpy array): nx4 array containing:\n",
        "            (a) in the first column, the DataFrame indices of the elements of the proposed 3-DDGP order_1; \n",
        "            (b) in the other three columns, the neigbours which are utilized to emerge the vertices in the R^3;\n",
        "                order_1[i][1] is closer to order_1[i][0] in the order than order_1[i][2] is, and so on.    \n",
        "    \"\"\"\n",
        "\n",
        "    # df['myind'] = [str(i) for i in range(len(df))]\n",
        "    df = df.reset_index()\n",
        "    df['myindex'] = df['index'].apply(lambda x : str(x))\n",
        "    df_residue_info = df.groupby(['residue_number']).agg({'residue_name': 'min', 'atom_name': lambda x : ' '.join(x), 'myindex': lambda y : ' '.join(y)}).reset_index()\n",
        "    df = df.drop(['myindex'], axis=1)\n",
        "\n",
        "    # atom_names_im1 = df_residue_info.iloc[0][2].split(' ')\n",
        "    atom_names_im1 = df_residue_info.iloc[0].iloc[2].split(' ')\n",
        "    # atom_indexes_im1 = [int(atom_index) for atom_index in df_residue_info.iloc[0][3].split(' ')]\n",
        "    atom_indexes_im1 = [int(atom_index) for atom_index in df_residue_info.iloc[0].iloc[3].split(' ')]\n",
        "    atoms_im1 = {atom_names_im1[k]: atom_indexes_im1[k] for k in range(len(atom_names_im1))}\n",
        "\n",
        "    # Creating the order_1\n",
        "    order_1 = np.zeros((len(df), 4))\n",
        "\n",
        "    # The first three elements of the order\n",
        "    order_1[0][0] = atoms_im1['N']\n",
        "    order_1[1][0] = atoms_im1['CA']\n",
        "    order_1[2][0] = atoms_im1['HA']\n",
        "\n",
        "    # The first three elements of the order are not calculated using previous neighbours.\n",
        "    order_1[0][1] = np.nan\n",
        "    order_1[1][1] = np.nan\n",
        "    order_1[2][1] = np.nan\n",
        "    order_1[0][2] = np.nan\n",
        "    order_1[1][2] = np.nan\n",
        "    order_1[2][2] = np.nan\n",
        "    order_1[0][3] = np.nan\n",
        "    order_1[1][3] = np.nan\n",
        "    order_1[2][3] = np.nan\n",
        "\n",
        "    current_i = 3\n",
        "    for ind, row in df_residue_info.iloc[1:].iterrows():\n",
        "        #atom_names_i = row[2].split(' ')\n",
        "        atom_names_i = row.iloc[2].split(' ')\n",
        "        # atom_indexes_i = [int(atom_index) for atom_index in row[3].split(' ')]\n",
        "        atom_indexes_i = [int(atom_index) for atom_index in row.iloc[3].split(' ')]\n",
        "        atoms_i = {atom_names_i[k]: atom_indexes_i[k] for k in range(len(atom_names_i))}\n",
        "\n",
        "        ################################\n",
        "        \n",
        "        if 'C' in atoms_im1:\n",
        "            order_1[current_i][0] = atoms_im1['C']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'C' does not exist in the %d-th residue!\" % ind-1)\n",
        "\n",
        "        order_1[current_i][1] = atoms_im1['N']\n",
        "        order_1[current_i][2] = atoms_im1['CA']\n",
        "        order_1[current_i][3] = atoms_im1['HA']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        thisH = ''\n",
        "        if 'H' in atoms_i:\n",
        "            thisH = 'H'\n",
        "        elif 'H1' in atoms_i:\n",
        "                thisH = 'H1'                \n",
        "        else:\n",
        "            raise Exception(\"Atom 'H' does not exist in the %d-th residue!\" % ind)\n",
        "        order_1[current_i + 1][0] = atoms_i[thisH]\n",
        "\n",
        "        order_1[current_i + 1][1] = atoms_im1['CA']\n",
        "        order_1[current_i + 1][2] = atoms_im1['C']\n",
        "        order_1[current_i + 1][3] = atoms_im1['HA']\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'N' in atoms_i:\n",
        "            order_1[current_i + 2][0] = atoms_i['N']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'N' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 2][1] = atoms_im1['CA']\n",
        "        order_1[current_i + 2][2] = atoms_im1['C']\n",
        "        order_1[current_i + 2][3] = atoms_i[thisH]\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'CA' in atoms_i:\n",
        "            order_1[current_i + 3][0] = atoms_i['CA']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'CA' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 3][1] = atoms_im1['C']\n",
        "        order_1[current_i + 3][2] = atoms_i['N']\n",
        "        order_1[current_i + 3][3] = atoms_i[thisH]\n",
        "\n",
        "        ################################\n",
        "\n",
        "        if 'HA' in atoms_i:\n",
        "            order_1[current_i + 4][0] = atoms_i['HA']\n",
        "        else:\n",
        "            raise Exception(\"Atom 'HA' does not exist in the %d-th residue!\" % ind)\n",
        "\n",
        "        order_1[current_i + 4][1] = atoms_i['N']\n",
        "        order_1[current_i + 4][2] = atoms_i['CA']\n",
        "        order_1[current_i + 4][3] = atoms_i[thisH]\n",
        "\n",
        "        ################################\n",
        "\n",
        "        atom_names_im1 = atom_names_i.copy()\n",
        "        atom_indexes_im1 = atom_indexes_i.copy()\n",
        "        atoms_im1 = atoms_i.copy()\n",
        "        \n",
        "        current_i = current_i + 5\n",
        "    \n",
        "    # The last element of the order\n",
        "    if 'C' in atoms_i:\n",
        "        order_1[current_i][0] = atoms_im1['C']\n",
        "    else:\n",
        "        raise Exception(\"Atom 'C' does not exist in the %d-th residue!\" % ind)\n",
        "    \n",
        "    # The neighbours of the last element\n",
        "    order_1[current_i][1] = atoms_i['N']\n",
        "    order_1[current_i][2] = atoms_i['CA']\n",
        "    order_1[current_i][3] = atoms_i['HA']\n",
        "\n",
        "    return order_1\n",
        "\n",
        "\n",
        "def calculate_b_column(df, order=[]):\n",
        "    \"\"\"\n",
        "    Calculate the binary column 'b' based on the 'x' column of the DataFrame.\n",
        "    Parameters:\n",
        "        df (DataFrame): DataFrame containing the 'x' column with 3D coordinates.\n",
        "        order (numpy array): nx4 array containing:\n",
        "            (a) in the first column, the DataFrame indices of the elements of the proposed 3-DDGP order_1; \n",
        "            (b) in the other three columns, the neigbours which are utilized to emerge the vertices in the R^3;\n",
        "                order_1[i][1] is closer to order_1[i][0] in the order than order_1[i][2] is, and so on.\n",
        "    Returns:\n",
        "        Series: Pandas Series containing the binary values for the 'b' column.\n",
        "    \"\"\"\n",
        "\n",
        "    # When no order is passed, we assume the DataFrame order is a 3-DMDGP order\n",
        "    if len(order) == 0:\n",
        "        col0 = list(range(len(df)))\n",
        "        col1 = list(range(-3, len(df)-3))\n",
        "        col2 = list(range(-2, len(df)-2))\n",
        "        col3 = list(range(-1, len(df)-1))\n",
        "        col1[0:3] = [np.nan, np.nan, np.nan]\n",
        "        col2[0:3] = [np.nan, np.nan, np.nan]\n",
        "        col3[0:3] = [np.nan, np.nan, np.nan]\n",
        "        order = np.array([col0, col1, col2, col3,])\n",
        "        order = np.transpose(order)\n",
        "\n",
        "    b_values = []\n",
        "    for i in range(len(df)):\n",
        "        if i < 3:\n",
        "            # Not enough points to define a plane\n",
        "            b_values.append(None)\n",
        "            continue\n",
        "        plane_points = np.array(df.iloc[order[i][1:4].astype(int)]['x'].tolist())\n",
        "        current_point = df.iloc[order[i][0].astype(int)]['x']\n",
        "        distance_to_plane = point_plane_distance(current_point, plane_points)\n",
        "        b_values.append(int(distance_to_plane >= 0))\n",
        "    return pd.Series(b_values)\n",
        "\n",
        "\n",
        "def write_binary(fn_binary, df, order, b_col):\n",
        "    df_binary = df[['residue_number', 'atom_name']].copy()\n",
        "    indices = np.zeros([order.shape[0], 1])\n",
        "    for i in range(order.shape[0]):\n",
        "        indices[int(order[i][0])] = i\n",
        "    df_binary['order'] = indices\n",
        "    df_binary.sort_values(by=['order'], inplace=True)\n",
        "    df_binary.drop(\"order\", axis='columns', inplace=True)\n",
        "\n",
        "    # Guarantees that new columns will not be sorted\n",
        "    df_binary.reset_index(inplace=True)\n",
        "    df_binary.rename(columns={'index': 'order'}, inplace=True)\n",
        "\n",
        "    df_binary['N_1'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 1].astype(int)))\n",
        "    df_binary['N_2'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 2].astype(int)))\n",
        "    df_binary['N_3'] = np.concatenate((np.array([np.nan, np.nan, np.nan]), order[3:, 3].astype(int)))\n",
        "    df_binary['b'] = b_col\n",
        "\n",
        "    df_binary.to_csv(fn_binary, index=False)\n",
        "\n",
        "\n",
        "def flip_b_column_if_needed(b_col):\n",
        "    \"\"\"\n",
        "    Flip the binary column 'b' if the 4th element (index 3) is 1.\n",
        "    Parameters:\n",
        "        b_col (Series): Panda Series containing the 'b' column.\n",
        "    Returns:\n",
        "        Series: Pandas Series containing the binary values for the 'b' column flipped around the fourth position.\n",
        "    \"\"\"\n",
        "    if len(b_col) > 3 and b_col.iloc[3] == 0: # Padronizando o quarto elemento como 1\n",
        "        b_col = b_col.apply(lambda x: 1 - x if x is not None else None)        \n",
        "    return b_col\n",
        "\n",
        "os.makedirs(wd_binary, exist_ok=True)\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "def process_file(fn):\n",
        "    try:\n",
        "        fn_segment = os.path.join(wd_segment, fn)\n",
        "        fn_binary = os.path.join(wd_binary, fn.split('.')[0] + '_binary' + '.csv')\n",
        "        df = pd.read_csv(fn_segment)\n",
        "        order_1 = construct_ddgp_order_1(df)\n",
        "        # Create col x\n",
        "        df['x'] = df.apply(lambda row: np.array([row['x_coord'], row['y_coord'], row['z_coord']]), axis=1)\n",
        "        # Calculate the 'b' column\n",
        "        b_col = calculate_b_column(df, order_1)\n",
        "        b_col = flip_b_column_if_needed(b_col)\n",
        "        write_binary(fn_binary, df, order_1, b_col)\n",
        "    except Exception as e:\n",
        "        return 'File %s: ' %fn + e.args[0]\n",
        "\n",
        "# set the number of threads to be one less than the number of cores\n",
        "num_threads = os.cpu_count() - 1\n",
        "\n",
        "# Get list of segment files\n",
        "segment_files = sorted(os.listdir(wd_segment))\n",
        "\n",
        "# Parallel processing\n",
        "print(f'Processing files {len(segment_files)} files with {num_threads} threads')\n",
        "with ProcessPoolExecutor(max_workers=num_threads) as executor:\n",
        "    results = list(tqdm(executor.map(process_file, segment_files), total=len(segment_files)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitsB8R_8ITr",
        "outputId": "6c8264a1-aa9f-47fd-8cae-33b1cf9aa020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 73649/73649 [02:06<00:00, 584.06it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# wd_binary = os.path.join(wd, 'binary')\n",
        "M = {'fn':[], 'n':[], 's':[], 'order': [], 'N_1': [], 'N_2': [], 'N_3': []}\n",
        "\n",
        "for pdb_code in tqdm(os.listdir(wd_binary)):\n",
        "    pdb_code = os.path.join(wd_binary, pdb_code)\n",
        "    df = pd.read_csv(pdb_code)\n",
        "    # convert from b (binary) to s (string)\n",
        "    s = ''.join(df['b'].dropna().astype(int).astype(str))\n",
        "    s = s[1:] # remove b_4\n",
        "    order = ''.join(df['order'].astype(int).astype(str))\n",
        "    N_1 = ' '.join(df['N_1'].dropna().astype(int).astype(str))\n",
        "    N_2 = ' '.join(df['N_2'].dropna().astype(int).astype(str))\n",
        "    N_3 = ' '.join(df['N_3'].dropna().astype(int).astype(str))\n",
        "    M['fn'].append(pdb_code)\n",
        "    M['s'].append(s)\n",
        "    M['n'].append(len(s))\n",
        "    M['order'].append(order)\n",
        "    M['N_1'].append(N_1)\n",
        "    M['N_2'].append(N_2)\n",
        "    M['N_3'].append(N_3)\n",
        "\n",
        "M = pd.DataFrame(M)\n",
        "M.to_csv('df_strings.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HySmxeu0ATel",
        "outputId": "3f99c856-6d7e-4a20-c347-2483c1016538"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 73649/73649 [02:13<00:00, 553.37it/s] \n"
          ]
        }
      ],
      "source": [
        "# create *.sol files\n",
        "import re\n",
        "\n",
        "# extract atoms coordinates\n",
        "def extract_coordinates(df):\n",
        "    if len(df['x']) > 0:\n",
        "        df['x'] = df['x'].apply(lambda x: eval(re.sub(\" +\", \",\", re.sub(r\"\\[\\s+\",\"[\", x))))\n",
        "        x = np.array(df['x'].to_list())\n",
        "        return x\n",
        "    return None\n",
        "\n",
        "os.makedirs(wd_sol, exist_ok=True)\n",
        "\n",
        "# OBS: The files of the 'binary' folder represent the solvable ddgp problems.\n",
        "# Since the files of the 'segment' folder might not have a correspondent file \n",
        "# in the 'binary' folder, we iterate over the 'binary' files.\n",
        "\n",
        "def process_file(fn):\n",
        "    fn_segment = os.path.join(wd_segment, os.path.split(fn)[1])\n",
        "    fn_segment = fn_segment.replace('_binary.csv','.csv')\n",
        "    fn_sol = os.path.join(wd_sol, os.path.split(fn_segment)[1])\n",
        "    fn_sol = fn_sol.replace('.csv','.sol')\n",
        "    # skip if the file already exists\n",
        "    if os.path.exists(fn_sol):\n",
        "        return\n",
        "    df = pd.read_csv(fn_segment)\n",
        "    df['x'] = df.apply(lambda row: str(np.array([row['x_coord'], row['y_coord'], row['z_coord']])), axis=1)\n",
        "    x = extract_coordinates(df)\n",
        "    np.savetxt(fn_sol, x)\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "num_threads = os.cpu_count() - 1\n",
        "with ProcessPoolExecutor(max_workers=num_threads) as executor:\n",
        "    results = list(tqdm(executor.map(process_file, M['fn']), total=len(M['fn'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/73649 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'flip_b_column_if_needed' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m b_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries([\u001b[38;5;28mint\u001b[39m(b_str[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(b_str))])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# flip the binary subsequence around the fourth atom if it is necessary.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m b_series_flipped \u001b[38;5;241m=\u001b[39m \u001b[43mflip_b_column_if_needed\u001b[49m(b_series)\n\u001b[1;32m     34\u001b[0m b_str_flipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(b_series_flipped\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m     35\u001b[0m b_str_flipped \u001b[38;5;241m=\u001b[39m b_str_flipped[\u001b[38;5;241m4\u001b[39m:]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flip_b_column_if_needed' is not defined"
          ]
        }
      ],
      "source": [
        "# Counting the number of fixed-size binary subsequences.\n",
        "import math\n",
        "\n",
        "# read df_string.csv \n",
        "# M = pd.read_csv('df_strings.csv')\n",
        "\n",
        "# establishes the size of binary subsequences to be collected as the numbers of consecutive residues in the pieces.\n",
        "res_quantities = [1, 2, 3, 4, 5]\n",
        "n_atoms_per_res = 5\n",
        "slice_sizes = [n_atoms_per_res * res_quantities[i] for i in range(len(res_quantities))]\n",
        "\n",
        "# get all the binary subsequences of each established size for each protein segment available.\n",
        "slices = []\n",
        "for _, row in tqdm(list(M.iterrows())):\n",
        "    b_full:str = row['s']\n",
        "    assert type(b) == str\n",
        "\n",
        "    pdb_code = os.path.basename(row['fn']).split('_')[0]            \n",
        "    for size in slice_sizes:    \n",
        "        # get the first binary subsequence from the current segment: the first 'size' atoms.        \n",
        "        slices.append((b_full[0:size], pdb_code))\n",
        "        \n",
        "        # get all the other binary subsequences from the current segment: stepping from a residue to the next residue (5-atoms steps).\n",
        "        for slice_start in range(5, len(b), 5):\n",
        "            # add the first three fixed atoms and the first branching atom (the fourth one in the order).\n",
        "            b = b_full[(slice_start - 4):(slice_start + size)]\n",
        "            b_series = pd.Series([int(b[i]) for i in b])\n",
        "            \n",
        "            # flip the binary subsequence around the fourth atom if it is necessary.\n",
        "            b_series = flip_b_column_if_needed(b_series)\n",
        "            b = ''.join(b_series.astype(str))\n",
        "            b = b[4:]\n",
        "\n",
        "            assert type(b) == str\n",
        "            slices.append((b, pdb_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb_code\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m slices\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b, pdb_code \u001b[38;5;129;01min\u001b[39;00m slices[size]: \n\u001b[1;32m      5\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(b)\n\u001b[1;32m      6\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(pdb_code)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# transform all the binary subsequence data into a dataframe\n",
        "df = {'b':[], 'pdb_code':[]}\n",
        "for size in slices.keys():\n",
        "    for b, pdb_code in slices[size]: \n",
        "        df['b'].append(b)\n",
        "        df['pdb_code'].append(pdb_code)\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "# add a column to the dataframe with the size of each binary subsequence pattern.\n",
        "df['size'] = df['b'].apply(lambda x : len(x))\n",
        "\n",
        "# count the frequency of each binary subsequence pattern that has been collected.\n",
        "df['count'] = 0\n",
        "df = df.groupby(['b', 'size', 'pdb_code'], sort=True).count().reset_index()\n",
        "\n",
        "df_size = df[['size','count']].groupby(['size'], sort=True).sum().reset_index()\n",
        "# convert from dataframe to dictionary, using 'size' as the key.\n",
        "df_size = df_size.set_index('size').T.to_dict('records')[0]\n",
        "\n",
        "# calculate the relative frequency of each binary subsequence pattern.\n",
        "df['relfreq'] = df.apply(lambda x : x['count']/df_size[x['size']], axis=1)\n",
        "\n",
        "# sort the dataframe by size and count.\n",
        "df.sort_values(by=['size', 'count'], inplace=True, ascending=False)\n",
        "\n",
        "# keep only the columns 'size', 'count', 'relfreq', and 'b'.\n",
        "df = df[['size', 'count', 'relfreq', 'pdb_code', 'b']]\n",
        "\n",
        "# save dataframe to csv\n",
        "df.to_csv('df_count_slices.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of the numbers of fixed-size binary subsequences.\n",
        "\n",
        "\n",
        "def accumulate(acc_variable, acc_value):\n",
        "    acc_variable = acc_variable + acc_value\n",
        "    return acc_variable\n",
        "\n",
        "\n",
        "df_frequency = pd.DataFrame(df_count_slices['size'])\n",
        "\n",
        "# indexing the binary subsequences from the most frequent to the less frequent and normalizing them to the (0,1] interval.\n",
        "indexes_to_plot = []\n",
        "sizes = df_count_slices['size'].unique()#.groupby(['size']).min()\n",
        "for size in sizes:\n",
        "    this_indexes = list(range(len(df_count_slices[df_count_slices['size'] == size])))\n",
        "    this_indexes = [x/len(this_indexes) for x in range(len(this_indexes))]\n",
        "    indexes_to_plot = indexes_to_plot + this_indexes\n",
        "\n",
        "df_frequency['index_to_plot'] = indexes_to_plot\n",
        "\n",
        "# calculate the relative accumulated frequency of the binary subsequences.\n",
        "acc_freq = []\n",
        "rel_acc_freq = []\n",
        "for size in sizes:\n",
        "    df_size = df_count_slices[df_count_slices['size'] == size]\n",
        "    N_freq = df_size['count'].sum()\n",
        "    this_acc_freq = df_size['count'].cumsum()\n",
        "    this_rel_acc_freq = this_acc_freq.copy()\n",
        "    this_rel_acc_freq = this_rel_acc_freq.apply(lambda x: x/N_freq)\n",
        "\n",
        "    acc_freq += this_acc_freq.to_list()\n",
        "    rel_acc_freq += this_rel_acc_freq.to_list()\n",
        "df_frequency['acc_frequency'] = acc_freq\n",
        "df_frequency['rel_acc_frequency'] = rel_acc_freq\n",
        "\n",
        "# constructing the  graphic 'Normalized Index' x 'Accumulated Probability'.\n",
        "g_freq = px.line(df_frequency, x='index_to_plot', y=\"rel_acc_frequency\", color=\"size\", labels={'index_to_plot': 'Normalized Index', 'rel_acc_frequency': 'Accumulated Probability'}, log_x=True, log_y=True, markers=True)\n",
        "g_freq.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graphic (Vertical) that compares BP costs (as the number of visited nodes of the binary tree) when:\n",
        "#   (a) BP executes a regular DFS to find a solution;\n",
        "#   (b) BP firstly verify the most frequent root-leaf paths of the BP tree.\n",
        "\n",
        "def count_visited_nodes_by_dfs(binary):\n",
        "    \"\"\"\n",
        "    # Calculate the number of the BP tree nodes that are explored by the DFS until it finds the solution.\n",
        "    Parameters:\n",
        "        binary: Binary string representing the solution: a root-leaf path in the BP tree.\n",
        "    Returns:\n",
        "        Integer: The number of visited nodes.\n",
        "    Note:\n",
        "        This calculation is done using the following equation:\n",
        "              DFS_cost(b) = n + sum_{j=1}^{n} < (b_{k})_{k=1}^{n+1-j} , (2^{n-i})_{i=j}^{n} >.\n",
        "        The symbol < , > represents the dot product of two vectors.\n",
        "        The element 'n' counts the number of nodes of the solution 'b'.\n",
        "        The j-th dot product of the sum counts the number of nodes of the BP tree visited by the DFS in the 'n+1-j'-th level.\n",
        "    \"\"\"\n",
        "    n_binary = len(binary)\n",
        "    binary_int = np.array([int(x) for x in binary])\n",
        "    exp_base_2 = np.array([2**i for i in range(n_binary-1, -1, -1)])\n",
        "    \n",
        "    return np.sum([np.sum(binary_int[0:n_binary-i] * exp_base_2[i:n_binary]) + 1 for i in range(n_binary)])\n",
        "\n",
        "df_cost = pd.DataFrame(df_count_slices['size'])\n",
        "df_cost['count'] = df_count_slices['count'].copy()\n",
        "\n",
        "# calculating the \"regular BP\" cost to find out the current binary is a valid solution.\n",
        "df_cost['dfs_cost'] = df_count_slices['b'].apply(lambda x: count_visited_nodes_by_dfs(x))\n",
        "\n",
        "# indexing the binary subsequences from the most frequent to the less frequent.\n",
        "indexes_to_plot = []\n",
        "sizes = df_count_slices['size'].unique()\n",
        "for size in sizes:\n",
        "    this_indexes = list(range(len(df_count_slices[df_count_slices['size'] == size])))\n",
        "    this_indexes = [x+1 for x in range(len(this_indexes))]\n",
        "    indexes_to_plot = indexes_to_plot + this_indexes\n",
        "df_cost['index_to_plot'] = indexes_to_plot\n",
        "\n",
        "# calculating the \"preferential BP\" cost: the cost to find out if one of the known binary subsequences corresponds to the geometry of the current protein piece.\n",
        "df_cost['pref_bin_cost'] = (df_cost['index_to_plot'].astype(int)) * df_cost['size'].astype(int)\n",
        "df_cost = df_cost.drop(['index_to_plot'], axis=1)\n",
        "\n",
        "# measuring how much more \"regular BP\" costs than \"preferential BP\" : dfs_cost / preferable_cost. We call this measurement \"speed up\".\n",
        "df_cost['speed_up'] = df_cost.apply(lambda row: row['dfs_cost'].astype(float)/row['pref_bin_cost'].astype(float), axis=1)\n",
        "\n",
        "# sorting the dataframe from the greatest 'cost speed_up' to the smallest.\n",
        "df_cost.sort_values(by=['size', 'speed_up'], inplace=True, ascending=True)\n",
        "\n",
        "# calculate the relative accumulated frequency of the binary subsequences.\n",
        "acc_freq = []\n",
        "rel_acc_freq = []\n",
        "for size in sizes:\n",
        "    df_size = df_cost[df_cost['size'] == size]\n",
        "    N_freq = df_size['count'].sum()\n",
        "    this_acc_freq = df_size['count'].cumsum()\n",
        "    this_rel_acc_freq = this_acc_freq.copy()\n",
        "    this_rel_acc_freq = this_rel_acc_freq.apply(lambda x: x/N_freq)\n",
        "\n",
        "    acc_freq += this_acc_freq.to_list()\n",
        "    rel_acc_freq += this_rel_acc_freq.to_list()\n",
        "\n",
        "df_cost['rel_acc_frequency'] = rel_acc_freq\n",
        "\n",
        "# add horizontal line y=log(1): highlight the cases where 'dfs_cost' and 'pref_bin_cost' have the same cost.\n",
        "df_cost.loc[len(df_cost.index)] = ['x=1', 1, 1, 1, 1, 0]\n",
        "df_cost.loc[len(df_cost.index)] = ['x=1', 1, 1, 1, 1, 1]\n",
        "\n",
        "# constructing the graphic (DFS_cost/FBS_cost) x Accumulated Prob.\n",
        "my_hue_order = list(df_count_slices['size'].unique())\n",
        "my_hue_order.insert(0, 'x=1')\n",
        "g_costs = sns.relplot(df_cost, kind=\"line\", x=\"speed_up\", y=\"rel_acc_frequency\", hue=\"size\", hue_order=my_hue_order, palette=sns.color_palette(\"dark\", n_colors+1), aspect=1.2)\n",
        "g_costs.set(xscale=\"log\")\n",
        "g_costs.set_axis_labels(x_var=\"Relative Cost\", y_var=\"Accumulated Probability\")\n",
        "plt.axvline(1, color=sns.color_palette(\"dark\", 1)[0], linestyle='--')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graphic (Horizontal) that compares BP costs (as the number of visited nodes of the binary tree) when:\n",
        "#   (a) BP executes a regular DFS to find a solution;\n",
        "#   (b) BP firstly verify the most frequent root-leaf paths of the BP tree.\n",
        "\n",
        "def count_visited_nodes_by_dfs(binary):\n",
        "    \"\"\"\n",
        "    # Calculate the number of the BP tree nodes that are explored by the DFS until it finds the solution.\n",
        "    Parameters:\n",
        "        binary: Binary string representing the solution: a root-leaf path in the BP tree.\n",
        "    Returns:\n",
        "        Integer: The number of visited nodes.\n",
        "    Note:\n",
        "        This calculation is done using the following equation:\n",
        "              DFS_cost(b) = n + sum_{j=1}^{n} < (b_{k})_{k=1}^{n+1-j} , (2^{n-i})_{i=j}^{n} >.\n",
        "        The symbol < , > represents the dot product of two vectors.\n",
        "        The element 'n' counts the number of nodes of the solution 'b'.\n",
        "        The j-th dot product of the sum counts the number of nodes of the BP tree visited by the DFS in the 'n+1-j'-th level.\n",
        "    \"\"\"\n",
        "    n_binary = len(binary)\n",
        "    binary_int = np.array([int(x) for x in binary])\n",
        "    exp_base_2 = np.array([2**i for i in range(n_binary-1, -1, -1)])\n",
        "    \n",
        "    return np.sum([np.sum(binary_int[0:n_binary-i] * exp_base_2[i:n_binary]) + 1 for i in range(n_binary)])\n",
        "\n",
        "df_cost = pd.DataFrame(df_count_slices['size'])\n",
        "df_cost['count'] = df_count_slices['count'].copy()\n",
        "\n",
        "# calculating the \"regular BP\" cost to find out the current binary is a valid solution.\n",
        "df_cost['dfs_cost'] = df_count_slices['b'].apply(lambda x: count_visited_nodes_by_dfs(x))\n",
        "\n",
        "# indexing the binary subsequences from the most frequent to the less frequent.\n",
        "indexes_to_plot = []\n",
        "sizes = df_count_slices['size'].unique()\n",
        "for size in sizes:\n",
        "    this_indexes = list(range(len(df_count_slices[df_count_slices['size'] == size])))\n",
        "    this_indexes = [x+1 for x in range(len(this_indexes))]\n",
        "    indexes_to_plot = indexes_to_plot + this_indexes\n",
        "df_cost['index_to_plot'] = indexes_to_plot\n",
        "\n",
        "# calculating the \"preferential BP\" cost: the cost to find out if one of the known binary subsequences corresponds to the geometry of the current protein piece.\n",
        "df_cost['pref_bin_cost'] = (df_cost['index_to_plot'].astype(int)) * df_cost['size'].astype(int)\n",
        "df_cost = df_cost.drop(['index_to_plot'], axis=1)\n",
        "\n",
        "# measuring how much more \"regular BP\" costs than \"preferential BP\" : dfs_cost / preferable_cost. We call this measurement \"speed up\".\n",
        "df_cost['speed_up'] = df_cost.apply(lambda row: row['dfs_cost'].astype(float)/row['pref_bin_cost'].astype(float), axis=1)\n",
        "\n",
        "# sorting the dataframe from the greatest 'cost speed_up' to the smallest.\n",
        "df_cost['size'] = df_cost['size'].apply(lambda x : -x) # keeps the dataframe ordered by size\n",
        "df_cost.sort_values(by=['size', 'speed_up'], inplace=True, ascending=False) # keeps the dataframe ordered by size\n",
        "df_cost['size'] = df_cost['size'].apply(lambda x : -x) # keeps the dataframe ordered by size\n",
        "\n",
        "# calculate the relative accumulated frequency of the binary subsequences.\n",
        "acc_freq = []\n",
        "rel_acc_freq = []\n",
        "for size in sizes:\n",
        "    df_size = df_cost[df_cost['size'] == size]\n",
        "    N_freq = df_size['count'].sum()\n",
        "    this_acc_freq = df_size['count'].cumsum()\n",
        "    this_rel_acc_freq = this_acc_freq.copy()\n",
        "    this_rel_acc_freq = this_rel_acc_freq.apply(lambda x: x/N_freq)\n",
        "\n",
        "    acc_freq += this_acc_freq.to_list()\n",
        "    rel_acc_freq += this_rel_acc_freq.to_list()\n",
        "\n",
        "df_cost['rel_acc_frequency'] = rel_acc_freq\n",
        "\n",
        "# add horizontal line y=log(1): highlight the cases where 'dfs_cost' and 'pref_bin_cost' have the same cost.\n",
        "df_cost.loc[len(df_cost.index)] = ['y=1', 1, 1, 1, 1, 0]\n",
        "df_cost.loc[len(df_cost.index)] = ['y=1', 1, 1, 1, 1, 1]\n",
        "\n",
        "# constructing the graphic Accumulated Prob x (DFS_cost/FBS_cost).\n",
        "my_hue_order = list(df_count_slices['size'].unique())\n",
        "my_hue_order.insert(0, 'y=1')\n",
        "g_costs = sns.relplot(df_cost, kind=\"line\", x=\"rel_acc_frequency\", y=\"speed_up\", hue=\"size\", hue_order=my_hue_order, palette=sns.color_palette(\"dark\", n_colors+1))\n",
        "g_costs.set(yscale=\"log\")\n",
        "g_costs.set_axis_labels(x_var=\"Accumulated Probability\", y_var=\"Relative Cost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a graphic that shows the ratio of the most frequent binary subsequence and the second most frequent.\n",
        "\n",
        "df_2most_freq = df_count_slices.groupby(['size']).agg({'count': 'max'}).reset_index()\n",
        "df_2most_freq = df_2most_freq.rename(columns={'count': 'big_1st'})\n",
        "\n",
        "most_freq_indices = [np.flatnonzero((df_count_slices['size'] == row.loc['size']) & (df_count_slices['count'] == row.loc['big_1st']))[0].astype(int) for _, row in df_2most_freq.iterrows()]\n",
        "df_2most_freq['big_2nd'] = [df_count_slices.iloc[i+1].loc['count'] for i in most_freq_indices]\n",
        "\n",
        "df_2most_freq['N_freq'] = df_count_slices.groupby(['size'])['count'].sum().reset_index()['count']\n",
        "\n",
        "df_2most_freq['rel_big_1st'] = df_2most_freq.apply(lambda row: row.loc['big_1st']/row.loc['N_freq'], axis=1)\n",
        "df_2most_freq['rel_big_2nd'] = df_2most_freq.apply(lambda row: row.loc['big_2nd']/row.loc['N_freq'], axis=1)\n",
        "\n",
        "df_2most_freq['ratio_1st_to_2nd'] = df_2most_freq.apply(lambda row: row.loc['rel_big_1st']/row.loc['rel_big_2nd'], axis=1)\n",
        "\n",
        "g_2most_freq = sns.barplot(df_2most_freq, x=\"size\", y=\"ratio_1st_to_2nd\", hue='size', palette=sns.color_palette(\"dark\",)[1:6], legend=False)\n",
        "g_2most_freq.set_ylabel(\"RF(1st_biggest) / RF(2nd_biggest)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
